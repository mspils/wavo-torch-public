{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14257a01-5277-46a9-870f-3a94af8990ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as data\n",
    "from torch import Tensor\n",
    "\n",
    "import os\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from functools import reduce\n",
    "\n",
    "from typing import Dict, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595a0f50-d64a-4840-a83e-be6c76de2b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "#for i in range(torch.cuda.device_count()):\n",
    "#    torch.cuda.set_per_process_memory_fraction(0.25, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4cff83-ec03-4552-8a68-208d04d5c080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(df:pd.DataFrame,max_fill=10):\n",
    "    \"\"\"Fills values in a DataFrame.\n",
    "    First columns recognized as Precipitation columns (containing NEW or NVh) are filled with 0,\n",
    "    other columns are filled linear with a limit of 24, with warning if more than 5 continuos values are missing\n",
    "\n",
    "    Args:\n",
    "        df : DataFrame that might be missing values\n",
    "        max_fill : How many continuosly missing values are allowed\n",
    "    Returns:\n",
    "        DataFrame: Filled DataFrame (if possible)\n",
    "    \"\"\"\n",
    "    old_size = df.shape[0]\n",
    "\n",
    "    df = df.resample('h').mean()\n",
    "    na_count = df.isna().sum(axis=0)\n",
    "\n",
    "    # get all columns with precipitation and fill missing values with 0\n",
    "    mask = df.columns.str.contains('NEW') | df.columns.str.contains('NVh')\n",
    "\n",
    "    prec_cols = list(na_count[mask][na_count[mask]>0].index)\n",
    "    if len(prec_cols) > 0:\n",
    "        df.loc[:, mask] = df.loc[:,mask].fillna(0)\n",
    "\n",
    "    # interpolate data in all other columns\n",
    "    df = df.interpolate(limit=max_fill,limit_direction='both')\n",
    "\n",
    "    if df.isna().sum().sum() > 0:\n",
    "        raise LargeGapError(f\"Some columns were missing more than {max_fill} continuous values, either raise the limit or fill values manually.\" +\n",
    "         f\"{df.isna().sum().sum()} still missing, maybe due to {len(df)-old_size} missing timestamps?\")\n",
    "    return df\n",
    "\n",
    "def maybe_add_differencing(df:pd.DataFrame,config:dict) -> Tuple[pd.DataFrame,str]:\n",
    "    \"\"\"If differencing is set to 1 in the config then a differenced column called 'd1' is added to df.\n",
    "    This also means that the name of the target column is changed from the original gaugename to 'd1'\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): input data\n",
    "        config (dict): dictionary containing all hyperparameters\n",
    "\n",
    "    Returns:\n",
    "        tuple[pd.DataFrame,str]: potentially changed input data and the name of the target column\n",
    "    \"\"\"\n",
    "    if config['differencing'] == 1:\n",
    "        config['level_name_tar'] = 'd1'\n",
    "        df[config['level_name_tar']] = df[config['level_name_org']].diff()\n",
    "        df = df.dropna()\n",
    "        gauge = config['level_name_tar']\n",
    "    else:\n",
    "        config['level_name_tar'] = config['level_name_org']\n",
    "        gauge = config['level_name_org']\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_cutoff_points(config: dict,df_len: int) -> Tuple[int,int,int,int]:\n",
    "    \"\"\"Calculates the cutoff points for training, validation and test set with or without cross validation\n",
    "\n",
    "    Args:\n",
    "        config (dict): dictionary containing all hyperparameters\n",
    "        df_len (int): lenght of the input dataframe\n",
    "\n",
    "    Returns:\n",
    "        tuple[int,int,int,int]: start_train,cut_off_train,cut_off_val,start_test\n",
    "    \"\"\"\n",
    "    start_test = int((1-config['test'])*df_len)\n",
    "    if 'cross_i' in config and 'cross_n' in config and config['cross_i'] < config['cross_n']:\n",
    "        cv_size = int(start_test / (config['cross_n']+1))\n",
    "        start_train = config['cross_i']*cv_size\n",
    "        cut_off_train = start_train + cv_size\n",
    "        cut_off_val = cut_off_train + cv_size\n",
    "    else:\n",
    "        start_train = 0\n",
    "        cut_off_train = int(config['train']*df_len)\n",
    "        #cut_off_val =cut_off_train + int(config['val']*df_len) 'TODO is this correct?\n",
    "        cut_off_val =int(config['train']*df_len + config['val']*df_len)\n",
    "\n",
    "    assert cut_off_val == start_test\n",
    "    assert cut_off_val <= start_test\n",
    "    return start_train,cut_off_train,cut_off_val\n",
    "\n",
    "def create_dataset(df,target_column,in_size=144,out_size=48):\n",
    "    \"\"\"Transform a time series into a prediction dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset: A numpy array of time series, first dimension is the time steps\n",
    "        lookback: Size of window for prediction\n",
    "    \"\"\"\n",
    "    dataset = df.values\n",
    "\n",
    "    #if config['differencing'] == 1:\n",
    "    target_idx = df.columns.get_loc(target_column)\n",
    "    \n",
    "    X, y = [], []\n",
    "    X2, y2 = [], []\n",
    "    for i in range(len(dataset)-in_size-out_size):\n",
    "        feature = dataset[i:i+in_size]\n",
    "        target = dataset[i+in_size:i+out_size+in_size,target_idx]\n",
    "        X.append(feature)\n",
    "        y.append(target)\n",
    "        \n",
    "    X = np.array(X)\n",
    "    y = np.array(y) \n",
    "    return torch.tensor(X,dtype=torch.float32), torch.tensor(y,dtype=torch.float32)\n",
    "\n",
    "def get_objective_metric(s):\n",
    "    if s in ['nse','kge']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "        \n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    " \n",
    "  def __init__(self,X,y):\n",
    "    self.X=X\n",
    "    self.y=y\n",
    " \n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "   \n",
    "  def __getitem__(self,idx):\n",
    "    return self.X[idx],self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b66c0e9-e79f-4e48-bf7c-dc1fbd83d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'filename' : '../../data/input/FoehrdenBarl3.csv',\n",
    "        'train':0.7,\n",
    "        'val':0.15,\n",
    "        'test':0.15,\n",
    "        #'batch_size':128,\n",
    "        'in_size':144,\n",
    "        'out_size':48,\n",
    "        'scale_target':False,\n",
    "        'level_name_org': 'FoehrdenBarl_pegel_cm',\n",
    "        #'differencing':1,\n",
    "        'differencing':0,\n",
    "        'percentile' :0.95\n",
    "         }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db45998c-681d-4e75-a689-a5389acaa98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaVoDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, filename,level_name_org,in_size=144,out_size=48,batch_size=128,differencing=0,percentile=0.95,train=0.7,val=.15,test=.15,**kwargs):\n",
    "        super().__init__()\n",
    "        self.filename = filename\n",
    "        #self.batch_size = batch_size\n",
    "        self.gauge_column = level_name_org\n",
    "        self.target_column = level_name_org\n",
    "        self.in_size= in_size\n",
    "        self.out_size = out_size \n",
    "        self.differencing = differencing\n",
    "        self.percentile = percentile\n",
    "        self.train = train\n",
    "        self.val = val\n",
    "        self.test = test\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        df = pd.read_csv(self.filename,index_col=0,parse_dates=True)\n",
    "        df = fill_missing_values(df)\n",
    "\n",
    "        if self.differencing == 1:\n",
    "            self.target_column = 'd1'\n",
    "            df[self.target_column] = df[self.gauge_column].diff()\n",
    "            \n",
    "        df = df[1:] #drop the first value, it's nan if differencing, but for comparing we need to always drop it\n",
    "        if 0 < self.percentile < 1:\n",
    "            self.threshold = df[self.gauge_column].quantile(self.percentile).item()\n",
    "        else:\n",
    "            self.threshold=self.percentile\n",
    "\n",
    "\n",
    "        val_idx = int(self.train*len(df))\n",
    "        test_idx = int(val_idx + self.val*len(df))\n",
    "        train, val, test = df[:val_idx], df[val_idx:test_idx], df[test_idx:]\n",
    "        \n",
    "        _,y_train_temp = self.create_dataset(train)\n",
    "        \n",
    "        ss_target = StandardScaler()\n",
    "        ss_target.fit(y_train_temp)\n",
    "        self.mean = ss_target.mean_.mean().item() #TODO use actual scalers?\n",
    "        self.scale = ss_target.scale_.mean().item()\n",
    "\n",
    "        ss = StandardScaler()\n",
    "        train_ss = pd.DataFrame(ss.fit_transform(train),index=train.index,columns=train.columns)\n",
    "        val_ss = pd.DataFrame(ss.transform(val),index=val.index,columns=val.columns)\n",
    "        test_ss = pd.DataFrame(ss.transform(test),index=test.index,columns=test.columns)\n",
    "        \n",
    "        X_train,self.y_train = self.create_dataset(train_ss)\n",
    "        X_val,  self.y_val =     self.create_dataset(val_ss)\n",
    "        X_test, self.y_test =   self.create_dataset(test_ss)\n",
    "        self.feature_count = X_train.shape[-1] #TODO config['feature_count']\n",
    "\n",
    "        \n",
    "        self.train_set = MyDataset(X_train,self.y_train)\n",
    "        self.val_set = MyDataset(X_val,self.y_val)\n",
    "        self.test_set = MyDataset(X_test,self.y_test)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set,batch_size=self.hparams.batch_size,shuffle=True,num_workers=8)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_set,batch_size=self.hparams.batch_size,shuffle=False,num_workers=8)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_set,batch_size=self.hparams.batch_size,shuffle=False,num_workers=8)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        #return the trainset, but sorted\n",
    "        return DataLoader(self.train_set,batch_size=self.hparams.batch_size,shuffle=False,num_workers=8)\n",
    "\n",
    "    def create_dataset(self,df):\n",
    "        dataset = df.values\n",
    "            \n",
    "        target_idx = df.columns.get_loc(self.target_column)\n",
    "        \n",
    "        X, y = [], []\n",
    "        X2, y2 = [], []\n",
    "        for i in range(len(dataset)-self.in_size-self.out_size):\n",
    "            feature = dataset[i:i+self.in_size]\n",
    "            target = dataset[i+self.in_size:i+self.out_size+self.in_size,target_idx]\n",
    "            X.append(feature)\n",
    "            y.append(target)\n",
    "            \n",
    "        X = np.array(X)\n",
    "        y = np.array(y) \n",
    "        return torch.tensor(X,dtype=torch.float32), torch.tensor(y,dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7576cd-af68-4052-8ae3-64b3901ba872",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c358b38-3744-4d4a-a855-0f264b95a217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7189f170-7df6-443d-9e71-882bb9441243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nse_series(y_true: Tensor,y_pred: Tensor):\n",
    "\n",
    "    nom = torch.sum(torch.square(torch.sub(y_true,y_pred)),axis=0)\n",
    "    denom = torch.sum(torch.square(torch.sub(y_true,torch.mean(y_true,axis=0))),axis=0)\n",
    "\n",
    "    return 1 - (nom / denom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35542123-a417-4fa8-ac52-69cdf5e01f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, feature_count,in_size=144,out_size=48,hidden_size_lstm=128,hidden_size=64,num_layers=2,dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=feature_count, hidden_size=hidden_size_lstm,\n",
    "                            num_layers=num_layers, batch_first=True, dropout=dropout) # lstm\n",
    "        self.fc_1 =  nn.Linear(hidden_size_lstm, hidden_size) # fully connected \n",
    "        self.fc_2 = nn.Linear(hidden_size*in_size,out_size) # fully connected last layer\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        output, _ = self.lstm(x) # (input, hidden, and internal state)\n",
    "        out = self.relu(output)\n",
    "        out = self.fc_1(out) # first dense\n",
    "        out = self.relu(out) # relu\n",
    "        out = self.flatten(out) #flatten\n",
    "        out = self.fc_2(out) # final output\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f42dbe-1352-4ebc-a52c-c5cee12b3642",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import Callback\n",
    "\n",
    "\n",
    "class MyEvaluationCallback(Callback):\n",
    "\n",
    "    #def __init__(self,train_loader,train_true,val_loader,val_true,test_loader,test_true,chosen_metrics= {'nse':nse_series}):\n",
    "    def __init__(self,chosen_metrics= {'nse':nse_series}):\n",
    "        super().__init__()\n",
    "        self.chosen_metrics = chosen_metrics\n",
    "\n",
    "    def on_train_start(self, trainer, pl_module):\n",
    "        # initialize metrics to log in the hyperparameter tab\n",
    "        #TODO update syntax when 3.9\n",
    "        metric_placeholders = {s:get_objective_metric(s) for s in self.chosen_metrics.keys()}\n",
    "        metric_placeholders = {**metric_placeholders,**{f'{k}_flood':v for k,v in metric_placeholders.items()}}\n",
    "        metric_placeholders = [{f'hp/{cur_set}_{k}':v for k,v in metric_placeholders.items()} for cur_set in ['train','val','test']] + [{'hp/val_loss':0,'hp/train_loss':0}]\n",
    "        metric_placeholders = reduce(lambda a, b: {**a, **b}, metric_placeholders)\n",
    "        pl_module.logger.log_hyperparams(pl_module.hparams,metric_placeholders)\n",
    "        \n",
    "    def on_fit_end(self, trainer, pl_module):\n",
    "        self.threshold = pl_module.threshold\n",
    "        metrics = {}\n",
    "        \n",
    "        y_true, y_pred,y_true_flood, y_pred_flood = self.get_eval_tensors(trainer,pl_module,trainer.datamodule.predict_dataloader(),trainer.datamodule.y_train)\n",
    "        for m_name, m_func in self.chosen_metrics.items():\n",
    "            metrics[f'hp/train_{m_name}'] = m_func(y_true,y_pred)\n",
    "            metrics[f'hp/train_{m_name}_flood'] = m_func(y_true_flood,y_pred_flood)\n",
    "\n",
    "        y_true, y_pred,y_true_flood, y_pred_flood = self.get_eval_tensors(trainer,pl_module,trainer.datamodule.val_dataloader(),trainer.datamodule.y_val)\n",
    "        for m_name, m_func in self.chosen_metrics.items():\n",
    "            metrics[f'hp/val_{m_name}'] = m_func(y_true,y_pred)\n",
    "            metrics[f'hp/val_{m_name}_flood'] = m_func(y_true_flood,y_pred_flood)\n",
    "\n",
    "        y_true, y_pred,y_true_flood, y_pred_flood = self.get_eval_tensors(trainer,pl_module,trainer.datamodule.test_dataloader(),trainer.datamodule.y_test)\n",
    "        for m_name, m_func in self.chosen_metrics.items():\n",
    "            metrics[f'hp/test_{m_name}'] = m_func(y_true,y_pred)\n",
    "            metrics[f'hp/test_{m_name}_flood'] = m_func(y_true_flood,y_pred_flood)\n",
    "\n",
    "\n",
    "        logger = trainer.logger\n",
    "        for i in range(pl_module.out_size):\n",
    "            logger.log_metrics({k:v[i] for k,v in metrics.items()},step=i+1)\n",
    "\n",
    "\n",
    "    def get_eval_tensors(self,trainer,pl_module,data_loader,y_true):\n",
    "        \n",
    "        y_true = pl_module.scale* y_true.cuda() + pl_module.mean\n",
    "\n",
    "        y_pred = trainer.predict(pl_module, data_loader)\n",
    "        y_pred = torch.concat(y_pred).cuda()\n",
    "        \n",
    "        mask = torch.any(torch.greater(y_true,self.threshold),axis=1)\n",
    "\n",
    "        y_true_flood = y_true[mask]\n",
    "        y_pred_flood = y_pred[mask]\n",
    "\n",
    "        return y_true, y_pred,y_true_flood, y_pred_flood\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b2ae40-414a-4171-928a-ab88904da80d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d3ee97-a07b-4441-9e74-9f054a40c724",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc21ed6c-1ddc-4526-b2e5-2d48a3c14c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# define the LightningModule\n",
    "class LSTMLightning(pl.LightningModule):\n",
    "    def __init__(self,mean,scale,threshold,feature_count,in_size=144,out_size=48,hidden_size_lstm=128,hidden_size=64,num_layers=2,dropout=0.2,learning_rate=0.001):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.mean = mean\n",
    "        self.scale = scale\n",
    "        self.threshold = threshold\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "        self.lstm  = LSTM(\n",
    "            feature_count=feature_count,\n",
    "            in_size=in_size,\n",
    "            out_size=out_size,\n",
    "            hidden_size_lstm=128,\n",
    "            hidden_size=64)\n",
    "        #self.decoder = decoder\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.lstm(x)\n",
    "        loss = nn.functional.mse_loss(y_hat, y)\n",
    "        self.log(\"hp/train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.lstm(x)\n",
    "        val_loss = nn.functional.mse_loss(y_hat, y)\n",
    "        self.log(\"hp/val_loss\", val_loss)\n",
    "\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.lstm(x)\n",
    "        test_loss = nn.functional.mse_loss(y_hat, y)\n",
    "        self.log(\"hp/test_loss\", test_loss)\n",
    "\n",
    "    def on_fit_end(self,*args,**kwargs):\n",
    "        print('test')\n",
    "        print(args)\n",
    "        print(kwargs)\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        x,y = batch\n",
    "        pred =  self.lstm(x)\n",
    "        if batch_idx == 0 and isinstance(self.mean,float):\n",
    "            device = torch.device(pred.get_device())\n",
    "            self.mean = torch.tensor(self.mean,device=device)\n",
    "            self.scale = torch.tensor(self.scale,device=device)\n",
    "\n",
    "        pred = pred*self.scale + self.mean\n",
    "        return pred\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82688a44-5b0a-4b72-b4f0-97c80df38c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = WaVoDataModule(**config)\n",
    "data_module.prepare_data()\n",
    "data_module.setup(stage='fit')\n",
    "\n",
    "model  = LSTMLightning(\n",
    "    data_module.mean,\n",
    "    data_module.scale,\n",
    "    data_module.threshold,\n",
    "    feature_count=data_module.feature_count,\n",
    "    in_size=data_module.hparams.in_size,\n",
    "    out_size=data_module.hparams.out_size,\n",
    "    hidden_size_lstm=128,\n",
    "    hidden_size=64,)\n",
    "\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"hp/val_loss\", mode=\"min\")\n",
    "my_callback = MyEvaluationCallback(chosen_metrics = {'nse':nse_series})\n",
    "\n",
    "\n",
    "callbacks = [early_stop_callback,my_callback]\n",
    "\n",
    "\n",
    "\n",
    "logger = TensorBoardLogger(\"../../models_torch\",default_hp_metric=False)\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(default_root_dir=\"../../models_torch/\",\n",
    "                     logger=logger,\n",
    "                     #accelerator=\"auto\",\n",
    "                     accelerator=\"gpu\",\n",
    "                     devices=1,\n",
    "                     #fast_dev_run=True,\n",
    "                     callbacks=callbacks,\n",
    "                     max_epochs=2)\n",
    "\n",
    "trainer.fit(model, data_module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f64cab5-6a67-47af-b678-ae73e55d8972",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d41c1e-2c51-49df-9411-cc40466feb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = pl.tuner.Tuner(trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d884521-0669-4128-bccb-d41f9a7b15b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f14bcd-5a8b-4810-b931-ac8d36bedb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.scale_batch_size(model, datamodule=data_module, init_val=2,max_trials=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6a18b5-4759-4ad1-a822-3194494dc257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2567243b-3af5-420d-b649-7b8b77cb73fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005b972a-43f3-433a-a3f6-5e383160bb4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8af869-8061-42a8-9795-a1bf13399596",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#trainer.fit(model, train_loader, val_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a17938-5c74-4b69-afbe-02bd494b411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0474846-7bd2-4df7-9727-805a15096db4",
   "metadata": {},
   "outputs": [],
   "source": [
    " tuner = pl.tuner.Tuner(trainer)\n",
    "    tuner.scale_batch_size(model, datamodule=data_module,\n",
    "                           init_val=512, max_trials=3)\n",
    "\n",
    "    print('start lr find')\n",
    "    tuner.lr_find(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8272ec3c-2cdc-45d2-8e0a-a8e525e85b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f11f389-bda7-4082-bcf2-6420a5246094",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logger.__dir__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d19f6ed-1719-4936-a599-427f50b4bcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.log_metrics({'mymetric' : [0,1,2,3,4,5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e312faa-19a5-4134-85e6-47686ed1ac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = logger.experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdfb151-afd7-45e9-96b1-a0e4b60675df",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.log_metrics(metrics={'p10' : 10,'p20' : 4},step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0dc0e2-5bc0-499d-9a7b-07c7696394d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c05486-79e1-49b4-8afc-25f02cf386bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e474386-6ab4-458d-9c91-c93c6ad40a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2487c687-8ce1-40b3-9381-8eef963a7830",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = trainer.predict(model, data_module.val_dataloader())\n",
    "y_pred = np.concatenate(y_pred)\n",
    "#y_true = ss_target.inverse_transform(y_val)\n",
    "y_true = data_module.y_val * data_module.scale + data_module.mean#mean = torch.Tensor([ss_target.mean_.mean()])\n",
    "#scale = torch.Tensor([ss_target.scale_.mean()])\n",
    "y_true2 = y_val * scale + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f5518f-1c1e-475a-bada-41f9426faa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = Tensor(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63885d42-09dd-4ee8-8dd9-f6cee621f17d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss = [0.5312353673607406, 0.6907600007921864, 0.7733655237945706, 0.8235671402540273, 0.8535743314519036, 0.8726369937689137, 0.8900624847467764, 0.9023950158692803, 0.9123187136231523, 0.9207956115188775, 0.9280167900267292, 0.9374969625189133, 0.9416359273491987, 0.9445962119935005, 0.9490530545548312, 0.953115569705147, 0.9557611738582984, 0.9585271997179228, 0.9606300142533792, 0.9639728842668186, 0.9655917414615075, 0.9695855739962049, 0.9704378314800728, 0.9704865778405427, 0.9706500615432333, 0.9731180767200983, 0.9749810049986383, 0.9774206582293399, 0.9783732937940121, 0.9793289752144098, 0.981572239453877, 0.981637686529275, 0.98430119485042, 0.9849038600461172, 0.9861981468914169, 0.9878829754440781, 0.9888318100695024, 0.9885800097663325, 0.9892959233493627, 0.9912060930582638, 0.990196691261009, 0.9902164774495048, 0.9885448924947428, 0.9901956789381205, 0.9901380208597038, 0.9896449689608526, 0.9895829397107665, 0.9894885912394472, 0.9889886901207936, 0.988439783224243, 0.9885874397195811, 0.9879768036694597, 0.9862999822997044, 0.9842079735256418, 0.9801386405311632, 0.9741757361540484, 0.9663148533761147, 0.9559287345311956, 0.9403883126460145, 0.9204661473784017, 0.9016760637603851, 0.8915467422393021, 0.8782093464600285, 0.8600765469729407, 0.8412999864842641, 0.8249062436890364, 0.8101222899712659, 0.7952404795001272, 0.7791314529846348, 0.7613936358821879, 0.7483093287540865, 0.7331612340852496, 0.7199459718162194, 0.70603820901942, 0.6917785837475893, 0.6801745804175745, 0.6703579572370906, 0.6616873340795681, 0.6497191085082195, 0.636334720936557, 0.6246159931366269, 0.6189680275218732, 0.609230812084805, 0.8110893209942514, 1.186254797541003, 1.1865623685747169, 18.37534764542335]\n",
    "lr = [1e-08, 1.4454397707459274e-08, 1.7378008287493753e-08, 2.0892961308540398e-08, 2.51188643150958e-08, 3.019951720402016e-08, 3.630780547701014e-08, 4.36515832240166e-08, 5.248074602497726e-08, 6.309573444801934e-08, 7.585775750291837e-08, 9.120108393559096e-08, 1.0964781961431852e-07, 1.3182567385564074e-07, 1.5848931924611133e-07, 1.9054607179632475e-07, 2.2908676527677735e-07, 2.7542287033381663e-07, 3.311311214825911e-07, 3.9810717055349735e-07, 4.786300923226383e-07, 5.75439937337157e-07, 6.918309709189366e-07, 8.317637711026709e-07, 1e-06, 1.2022644346174132e-06, 1.445439770745928e-06, 1.7378008287493761e-06, 2.089296130854039e-06, 2.5118864315095797e-06, 3.0199517204020163e-06, 3.630780547701014e-06, 4.365158322401661e-06, 5.248074602497728e-06, 6.3095734448019305e-06, 7.585775750291836e-06, 9.120108393559096e-06, 1.0964781961431852e-05, 1.3182567385564076e-05, 1.584893192461114e-05, 1.9054607179632464e-05, 2.2908676527677725e-05, 2.7542287033381663e-05, 3.311311214825911e-05, 3.9810717055349735e-05, 4.786300923226385e-05, 5.7543993733715664e-05, 6.918309709189363e-05, 8.317637711026709e-05, 0.0001, 0.00012022644346174131, 0.0001445439770745928, 0.00017378008287493763, 0.0002089296130854041, 0.0002511886431509582, 0.0003019951720402019, 0.000363078054770101, 0.0004365158322401656, 0.0005248074602497723, 0.000630957344480193, 0.0007585775750291836, 0.0009120108393559097, 0.0010964781961431851, 0.0013182567385564075, 0.001584893192461114, 0.0019054607179632484, 0.0022908676527677745, 0.002754228703338169, 0.003311311214825908, 0.003981071705534969, 0.00478630092322638, 0.005754399373371567, 0.006918309709189364, 0.008317637711026709, 0.01, 0.012022644346174132, 0.01445439770745928, 0.017378008287493765, 0.02089296130854041, 0.025118864315095822, 0.030199517204020192, 0.036307805477010104, 0.04365158322401657, 0.05248074602497723, 0.0630957344480193, 0.07585775750291836, 0.09120108393559097]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16235a29-08d9-4deb-9488-b5d4d2ff06b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'loss':loss,'lr':lr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0486f98-ce39-4143-b64f-a2b1a47970ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).plot(x='lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22988d8-46ec-4dd5-9c7d-c9bcf0cdba67",
   "metadata": {},
   "outputs": [],
   "source": [
    "2**11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aa29e6-8ffa-4eb5-8cda-09f2970c6cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = Tensor(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88515220-e3de-4c49-89dc-76b6dc356477",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7754a009-6dbb-4d32-a1ec-30351555c987",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce18ccc-494a-4cef-8778-a8c6efd731ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tf.reduce_mean(tf.square(tf.subtract(y_true,y_pred)),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942338ba-e95b-42f7-b407-486e58cda3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(torch.Tensor(y_true),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61481e0-d25c-4a2b-8218-25b782af6570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6f46a9-dcea-4525-99be-4d247c40650f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a23e02-d57f-409c-8c59-a50e2a7d1e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(config['filename'],index_col=0,parse_dates=True)\n",
    "df = fill_missing_values(df)\n",
    "df = maybe_add_differencing(df,config)\n",
    "_, val_idx,test_idx = get_cutoff_points(config,len(df))\n",
    "train, val, test = df[:val_idx], df[val_idx:test_idx], df[test_idx:]\n",
    "\n",
    "_,y_train_temp = create_dataset(train,config['level_name_target'],config['in_size'],config['out_size']])\n",
    "\n",
    "ss_target = StandardScaler()\n",
    "ss_target.fit(y_train_temp)\n",
    "\n",
    "ss = StandardScaler()\n",
    "train_ss = pd.DataFrame(ss.fit_transform(train),index=train.index,columns=train.columns)\n",
    "val_ss = pd.DataFrame(ss.transform(val),index=val.index,columns=val.columns)\n",
    "test_ss = pd.DataFrame(ss.transform(test),index=test.index,columns=test.columns)\n",
    "\n",
    "X_train,y_train = create_dataset(train_ss,config['level_name_target'],config['in_size'],config['out_size']])\n",
    "X_val,y_val = create_dataset(val_ss,config['level_name_target'],config['in_size'],config['out_size']])\n",
    "X_test,y_test = create_dataset(test_ss,config['level_name_target'],config['in_size'],config['out_size']])\n",
    "config['feature_count'] = X_train.shape[-1]\n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "train_set = MyDataset(X_train,y_train)\n",
    "val_set = MyDataset(X_val,y_val)\n",
    "test_set = MyDataset(X_test,y_test)\n",
    "\n",
    "train_loader = DataLoader(train_set,batch_size=128,shuffle=True,num_workers=8)\n",
    "train_loader_sorted = DataLoader(train_set,batch_size=128,shuffle=False,num_workers=8)\n",
    "val_loader = DataLoader(val_set,batch_size=128,shuffle=False,num_workers=8)\n",
    "test_loader = DataLoader(test_set,batch_size=128,shuffle=False,num_workers=8)\n",
    "\n",
    "threshold = df[config['level_name_org']].quantile(config['percentile'])\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
